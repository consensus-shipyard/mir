/*
Copyright IBM Corp. All Rights Reserved.

SPDX-License-Identifier: Apache-2.0

Refactored: 1
*/

package mir

import (
	"context"
	"fmt"
	"runtime/debug"

	"github.com/filecoin-project/mir/pkg/events"
	"github.com/filecoin-project/mir/pkg/pb/eventpb"
	"github.com/filecoin-project/mir/pkg/pb/statuspb"
	t "github.com/filecoin-project/mir/pkg/types"
)

// Input and output channels for the modules within the Node.
// the Node.process() method reads and writes events
// to and from these channels to rout them between the Node's modules.
type workChans struct {

	// There is one channel per module to feed events into the module.
	clients  chan *events.EventList
	protocol chan *events.EventList
	wal      chan *events.EventList
	hash     chan *events.EventList
	crypto   chan *events.EventList
	net      chan *events.EventList
	app      chan *events.EventList
	reqStore chan *events.EventList
	timer    chan *events.EventList

	// All modules write their output events in a common channel, from where the node processor reads and redistributes
	// the events to their respective workItems buffers.
	// External events are also funneled through this channel towards the workItems buffers.
	workItemInput chan *events.EventList

	// Events received during debugging through the Node.Step function are written to this channel
	// and inserted in the event loop.
	debugIn chan *events.EventList

	// During debugging, Events that would normally be inserted in the workItems event buffer
	// (and thus inserted in the event loop) are written to this channel instead if it is not nil.
	// If this channel is nil, those Events are discarded.
	debugOut chan *events.EventList
}

// Allocate and return a new workChans structure.
func newWorkChans() workChans {
	return workChans{
		clients:  make(chan *events.EventList),
		protocol: make(chan *events.EventList),
		wal:      make(chan *events.EventList),
		hash:     make(chan *events.EventList),
		crypto:   make(chan *events.EventList),
		net:      make(chan *events.EventList),
		app:      make(chan *events.EventList),
		reqStore: make(chan *events.EventList),
		timer:    make(chan *events.EventList),

		workItemInput: make(chan *events.EventList),

		debugIn:  make(chan *events.EventList),
		debugOut: make(chan *events.EventList),
	}
}

// A function type used for performing the work of a module.
// It usually reads events from a work channel and writes the output to another work channel.
// Any error that occurs while performing the work is returned.
// When ctx is canceled, the function should return ErrStopped
type workFunc func(ctx context.Context) error

// Calls the passed work function repeatedly in an infinite loop until the work function returns an non-nil error.
// doUntilErr then sets the error in the Node's workErrNotifier and returns.
func (n *Node) doUntilErr(ctx context.Context, work workFunc) {
	for {
		err := work(ctx)
		if err != nil {
			n.workErrNotifier.Fail(err)
			return
		}
	}
}

// eventProcessor defines the type of the function that processes a single input events.EventList,
// producing a single output events.EventList.
// There is one such function defined for each Module that is executed in a loop by a worker goroutine.
type eventProcessor func(context.Context, *events.EventList) (*events.EventList, error)

// processEvents reads a single list of input Events from a work channel, strips off all associated follow-up Events,
// and processes the bare content of the list using the passed processing function.
// processEvents writes all the stripped off follow-up events along with any Events generated by the processing
// to the workItemInput channel, so they will be added to the workItems buffer for further processing.
//
// If the Node is configured to use an Interceptor, after having removed all follow-up Events,
// processEvents passes the list of input Events to the Interceptor.
//
// If exitC is closed, returns ErrStopped.
func (n *Node) processEvents(
	ctx context.Context,
	processFunc eventProcessor,
	eventSource <-chan *events.EventList,
) error {
	var eventsIn *events.EventList

	// Read input.
	select {
	case eventsIn = <-eventSource:
	case <-ctx.Done():
		return ErrStopped
	}

	// Remove follow-up Events from the input EventList,
	// in order to re-insert them in the processing loop after the input events have been processed.
	plainEvents, followUps := eventsIn.StripFollowUps()

	// Intercept the (stripped of all follow-ups) events that are about to be processed.
	// This is only for debugging / diagnostic purposes.
	n.interceptEvents(plainEvents)

	// Process events.
	newEvents, err := processFunc(ctx, plainEvents)
	if err != nil {
		return fmt.Errorf("could not process events: %w", err)
	}

	// Merge the pending follow-up Events with the newly generated Events.
	out := followUps.PushBackList(newEvents)

	// Return if no output was generated.
	// This is only an optimization to prevent the processor loop from handling empty EventLists.
	if out.Len() == 0 {
		return nil
	}

	// Write output.
	select {
	case n.workChans.workItemInput <- out:
	case <-ctx.Done():
		return ErrStopped
	}

	return nil
}

// Module-specific wrappers for Node.ProcessEvents,
// associating each Module's processing function with its corresponding work channel.
// On top of that, the Protocol processing wrapper additionally sets the Node's exit status when done.

func (n *Node) doWALWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processWALEvents, n.workChans.wal)
}

func (n *Node) doClientWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processClientEvents, n.workChans.clients)
}

func (n *Node) doHashWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processHashEvents, n.workChans.hash)
}

func (n *Node) doCryptoWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processCryptoEvents, n.workChans.crypto)
}

func (n *Node) doSendingWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processSendEvents, n.workChans.net)
}

func (n *Node) doAppWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processAppEvents, n.workChans.app)
}

func (n *Node) doReqStoreWork(ctx context.Context) error {
	return n.processEvents(ctx, n.processReqStoreEvents, n.workChans.reqStore)
}

func (n *Node) doProtocolWork(ctx context.Context) (err error) {
	// On returning, sets the exit status of the protocol state machine in the work error notifier.
	defer func() {
		if err != nil {
			s, err := n.modules.Protocol.Status()
			n.workErrNotifier.SetExitStatus(&statuspb.NodeStatus{Protocol: s}, err)
			// TODO: Clean up status-related code.
		}
	}()
	return n.processEvents(ctx, n.processProtocolEvents, n.workChans.protocol)
}

func (n *Node) doTimerWork(ctx context.Context) (err error) {

	// Unlike other event processors that simply transform an event list to another event list,
	// the processor for the timer module needs direct access to the workItemsInput channel,
	// as the events it produces are (by definition) not available immediately.
	return n.processEvents(ctx, func(ctx context.Context, events *events.EventList) (*events.EventList, error) {
		return n.processTimerEvents(ctx, events, n.workChans.workItemInput)
	}, n.workChans.timer)
}

// TODO: Document the functions below.

func (n *Node) processWALEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {

	// If no WAL implementation is present, do nothing and return immediately.
	if n.modules.WAL == nil {
		return &events.EventList{}, nil
	}

	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()

	for event := iter.Next(); event != nil; event = iter.Next() {

		// Perform the necessary action based on event type.
		switch e := event.Type.(type) {
		case *eventpb.Event_WalAppend:
			if err := n.modules.WAL.Append(e.WalAppend.Event, t.WALRetIndex(e.WalAppend.RetentionIndex)); err != nil {
				return nil, fmt.Errorf("could not persist event (retention index %d) to WAL: %w",
					e.WalAppend.RetentionIndex, err)
			}
		case *eventpb.Event_WalTruncate:
			if err := n.modules.WAL.Truncate(t.WALRetIndex(e.WalTruncate.RetentionIndex)); err != nil {
				return nil, fmt.Errorf("could not truncate WAL (retention index %d): %w",
					e.WalTruncate.RetentionIndex, err)
			}
		case *eventpb.Event_PersistDummyBatch:
			if err := n.modules.WAL.Append(event, 0); err != nil {
				return nil, fmt.Errorf("could not persist dummy batch: %w", err)
			}
		default:
			return nil, fmt.Errorf("unexpected type of WAL event: %T", event.Type)
		}
	}

	// Then we sync the WAL
	if err := n.modules.WAL.Sync(); err != nil {
		return nil, fmt.Errorf("failed to sync WAL: %w", err)
	}

	return eventsOut, nil
}

func (n *Node) processClientEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {

	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		newEvents, err := n.safeApplyClientEvent(event)
		if err != nil {
			return nil, fmt.Errorf("err applying client event: %w", err)
		}
		eventsOut.PushBackList(newEvents)
	}

	return eventsOut, nil
}

func (n *Node) safeApplyClientEvent(event *eventpb.Event) (result *events.EventList, err error) {

	defer func() {
		if r := recover(); r != nil {
			if rErr, ok := r.(error); ok {
				err = fmt.Errorf("panic in client tracker: %w\nStack trace:\n%s", rErr, string(debug.Stack()))
			} else {
				err = fmt.Errorf("panic in client tracker: %v\nStack trace:\n%s", r, string(debug.Stack()))
			}
		}
	}()

	return n.modules.ClientTracker.ApplyEvent(event), nil
}

func (n *Node) processHashEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {
	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		switch e := event.Type.(type) {
		case *eventpb.Event_HashRequest:
			// HashRequest is the only event understood by the hasher module.

			// Create a slice for the resulting digests containing one element for each data item to be hashed.
			digests := make([][]byte, len(e.HashRequest.Data))

			// Hash each data item contained in the event
			for i, data := range e.HashRequest.Data {

				// One data item consists of potentially multiple byte slices.
				// Add each of them to the hash function.
				h := n.modules.Hasher.New()
				for _, d := range data.Data {
					h.Write(d)
				}

				// Save resulting digest in the result slice
				digests[i] = h.Sum(nil)
			}

			// Return all computed digests in one common event.
			eventsOut.PushBack(events.HashResult(digests, e.HashRequest.Origin))
		default:
			// Complain about all other incoming event types.
			return nil, fmt.Errorf("unexpected type of Hash event: %T", event.Type)
		}
	}

	return eventsOut, nil
}

func (n *Node) processCryptoEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {
	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		switch e := event.Type.(type) {
		case *eventpb.Event_VerifyRequestSig:
			// Verify client request signature.
			// The signature is only computed (and verified) over the digest of a request.
			// The other fields can safely be ignored.

			// Convenience variable
			reqRef := e.VerifyRequestSig.RequestRef

			// Verify signature.
			err := n.modules.Crypto.VerifyClientSig(
				[][]byte{reqRef.Digest},
				e.VerifyRequestSig.Signature,
				t.ClientID(reqRef.ClientId))

			// Create result event, depending on verification outcome.
			if err == nil {
				eventsOut.PushBack(events.RequestSigVerified(reqRef, true, ""))
			} else {
				eventsOut.PushBack(events.RequestSigVerified(reqRef, false, err.Error()))
			}
		case *eventpb.Event_SignRequest:
			// Compute a signature over the provided data and produce a SignResult event.

			if signature, err := n.modules.Crypto.Sign(e.SignRequest.Data); err == nil {
				eventsOut.PushBack(events.SignResult(signature, e.SignRequest.Origin))
			} else {
				return nil, err
			}
		case *eventpb.Event_VerifyNodeSigs:
			// Verify a batch of node signatures

			// Convenience variables
			verifyEvent := e.VerifyNodeSigs
			results := make([]bool, len(verifyEvent.Data))
			errors := make([]string, len(verifyEvent.Data))
			allOK := true

			// Verify each signature.
			for i, data := range verifyEvent.Data {
				err := n.modules.Crypto.VerifyNodeSig(data.Data, verifyEvent.Signatures[i], t.NodeID(verifyEvent.NodeIds[i]))
				if err == nil {
					results[i] = true
					errors[i] = ""
				} else {
					results[i] = false
					errors[i] = err.Error()
					allOK = false
				}
			}

			// Return result event
			eventsOut.PushBack(events.NodeSigsVerified(
				results,
				errors,
				t.NodeIDSlice(verifyEvent.NodeIds),
				verifyEvent.Origin,
				allOK,
			))

		default:
			// Complain about all other incoming event types.
			return nil, fmt.Errorf("unexpected type of Crypto event: %T", event.Type)
		}
	}

	return eventsOut, nil
}

func (n *Node) processSendEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {
	eventsOut := &events.EventList{}

	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		switch e := event.Type.(type) {
		case *eventpb.Event_SendMessage:
			for _, destID := range e.SendMessage.Destinations {
				if t.NodeID(destID) == n.ID {
					eventsOut.PushBack(events.MessageReceived(n.ID, e.SendMessage.Msg))
				} else {
					if err := n.modules.Net.Send(t.NodeID(destID), e.SendMessage.Msg); err != nil { // nolint
						// TODO: Handle sending errors (and remove "nolint" comment above).
					}
				}
			}
		default:
			return nil, fmt.Errorf("unexpected type of Net event: %T", event.Type)
		}
	}

	return eventsOut, nil
}

func (n *Node) processAppEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {
	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		switch e := event.Type.(type) {
		case *eventpb.Event_AnnounceDummyBatch:
			if err := n.modules.App.Apply(e.AnnounceDummyBatch.Batch); err != nil {
				return nil, fmt.Errorf("app error: %w", err)
			}
		case *eventpb.Event_Deliver:
			if err := n.modules.App.Apply(e.Deliver.Batch); err != nil {
				return nil, fmt.Errorf("app batch delivery error: %w", err)
			}
		case *eventpb.Event_AppSnapshotRequest:
			data, err := n.modules.App.Snapshot()
			if err != nil {
				return nil, fmt.Errorf("app snapshot error: %w", err)
			}
			eventsOut.PushBack(events.AppSnapshot(t.EpochNr(e.AppSnapshotRequest.Epoch), data))
		case *eventpb.Event_AppRestoreState:
			if err := n.modules.App.RestoreState(e.AppRestoreState.Data); err != nil {
				return nil, fmt.Errorf("app restore state error: %w", err)
			}
		default:
			return nil, fmt.Errorf("unexpected type of App event: %T", event.Type)
		}
	}

	return eventsOut, nil
}

func (n *Node) processReqStoreEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {
	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		// Process event based on its type.
		switch e := event.Type.(type) {
		case *eventpb.Event_StoreVerifiedRequest:
			storeEvent := e.StoreVerifiedRequest

			// Store request data.
			if err := n.modules.RequestStore.PutRequest(storeEvent.RequestRef, storeEvent.Data); err != nil {
				return nil, fmt.Errorf("cannot store request (c%vr%d) data: %w",
					storeEvent.RequestRef.ClientId,
					storeEvent.RequestRef.ReqNo,
					err)
			}

			// Mark request as authenticated.
			if err := n.modules.RequestStore.SetAuthenticated(storeEvent.RequestRef); err != nil {
				return nil, fmt.Errorf("cannot mark request (c%vr%d) as authenticated: %w",
					storeEvent.RequestRef.ClientId,
					storeEvent.RequestRef.ReqNo,
					err)
			}

			// Store request authenticator.
			if err := n.modules.RequestStore.PutAuthenticator(storeEvent.RequestRef, storeEvent.Authenticator); err != nil {
				return nil, fmt.Errorf("cannot store authenticator (c%vr%d) of request: %w",
					storeEvent.RequestRef.ClientId,
					storeEvent.RequestRef.ReqNo,
					err)
			}

		case *eventpb.Event_StoreDummyRequest:
			storeEvent := e.StoreDummyRequest // Helper variable for convenience

			// Store request data.
			if err := n.modules.RequestStore.PutRequest(storeEvent.RequestRef, storeEvent.Data); err != nil {
				return nil, fmt.Errorf("cannot store dummy request data: %w", err)
			}

			// Mark request as authenticated.
			if err := n.modules.RequestStore.SetAuthenticated(storeEvent.RequestRef); err != nil {
				return nil, fmt.Errorf("cannot mark dummy request as authenticated: %w", err)
			}

			// Associate a dummy authenticator with the request
			if err := n.modules.RequestStore.PutAuthenticator(storeEvent.RequestRef, []byte{0}); err != nil {
				return nil, fmt.Errorf("cannot store authenticator of dummy request: %w", err)
			}

			eventsOut.PushBack(events.RequestReady(storeEvent.RequestRef))
		}
	}

	// Then sync the request store, ensuring that all updates to its state are persisted.
	if err := n.modules.RequestStore.Sync(); err != nil {
		return nil, fmt.Errorf("could not sync request store, unsafe to continue: %w", err)
	}

	return eventsOut, nil
}

func (n *Node) processProtocolEvents(_ context.Context, eventsIn *events.EventList) (*events.EventList, error) {
	eventsOut := &events.EventList{}
	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		newEvents, err := n.safeApplyProtocolEvent(event)
		if err != nil {
			return nil, fmt.Errorf("error applying protocol event: %w", err)
		}
		eventsOut.PushBackList(newEvents)
	}

	return eventsOut, nil
}

func (n *Node) safeApplyProtocolEvent(event *eventpb.Event) (result *events.EventList, err error) {
	defer func() {
		if r := recover(); r != nil {
			if rErr, ok := r.(error); ok {
				err = fmt.Errorf("panic in protocol state machine: %w\nStack trace:\n%s", rErr, string(debug.Stack()))
			} else {
				err = fmt.Errorf("panic in protocol state machine: %v\nStack trace:\n%s", r, string(debug.Stack()))
			}
		}
	}()

	return n.modules.Protocol.ApplyEvent(event), nil
}

// processTimerEvents processes the events destined to the timer module.
// Unlike other event processors, processTimerEvents does not return a list of resulting events
// based on the input event list, since those are (by definition) delayed.
// The returned EventList is thus always empty.
// Instead, processTimerEvents receives an additional channel (notifyChan)
// where its outputs are written at the appropriate times.
func (n *Node) processTimerEvents(
	ctx context.Context,
	eventsIn *events.EventList,
	notifyChan chan<- *events.EventList,
) (*events.EventList, error) {

	iter := eventsIn.Iterator()
	for event := iter.Next(); event != nil; event = iter.Next() {

		// Based on event type, invoke the appropriate Timer function.
		// Note that events that later return to the event loop need to be copied in order to prevent a race condition
		// when they are later stripped off their follow-ups, as this happens potentially concurrently
		// with the original event being processed by the interceptor.
		switch e := event.Type.(type) {
		case *eventpb.Event_TimerDelay:
			n.modules.Timer.Delay(
				ctx,
				(&events.EventList{}).PushBackSlice(e.TimerDelay.Events),
				t.TimeDuration(e.TimerDelay.Delay),
				notifyChan,
			)
		case *eventpb.Event_TimerRepeat:
			n.modules.Timer.Repeat(
				ctx,
				(&events.EventList{}).PushBackSlice(e.TimerRepeat.Events),
				t.TimeDuration(e.TimerRepeat.Delay),
				t.TimerRetIndex(e.TimerRepeat.RetentionIndex),
				notifyChan,
			)
		case *eventpb.Event_TimerGarbageCollect:
			n.modules.Timer.GarbageCollect(t.TimerRetIndex(e.TimerGarbageCollect.RetentionIndex))
		default:
			return nil, fmt.Errorf("unexpected type of Timer event: %T", event.Type)
		}
	}

	return &events.EventList{}, nil
}
